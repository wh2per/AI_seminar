{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vQtKEP2tTU7",
        "colab_type": "code",
        "outputId": "f2948772-11e3-4aca-f1e8-05553df9f624",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# Char-RNN 예제\n",
        "# Author : solaris33\n",
        "# Project URL : http://solarisailab.com/archives/2487\n",
        "# GitHub Repository : https://github.com/solaris33/char-rnn-tensorflow/\n",
        "# Reference : https://github.com/sherjilozair/char-rnn-tensorflow\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import sys\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/AI_seminar_3/char-rnn-tensorflow-master')\n",
        "from utils import TextLoader\n",
        "\n",
        "# 학습에 필요한 설정값들을 지정합니다.\n",
        "data_dir = '/content/drive/My Drive/AI_seminar_3/char-rnn-tensorflow-master/data/tinyshakespeare'  # 셰익스피어 희곡 <리처드 3세> 데이터로 학습\n",
        "# data_dir = '/content/drive/My Drive/AI_seminar_3/char-rnn-tensorflow-master/data/linux' # <Linux 소스코드> 데이터로 학습\n",
        "#둘다 실습 돌려볼 것!\n",
        "batch_size = 50  # Training : 50, Sampling : 1\n",
        "seq_length = 50  # Training : 50, Sampling : 1\n",
        "hidden_size = 128  # 히든 레이어의 노드 개수\n",
        "learning_rate = 0.002\n",
        "num_epochs = 2\n",
        "num_hidden_layers = 2\n",
        "grad_clip = 5  # Gradient Clipping에 사용할 임계값\n",
        "\n",
        "# TextLoader를 이용해서 데이터를 불러옵니다.\n",
        "data_loader = TextLoader(data_dir, batch_size, seq_length)\n",
        "# 학습데이터에 포함된 모든 단어들을 나타내는 변수인 chars와 chars에 id를 부여해 dict 형태로 만든 vocab을 선언합니다.\n",
        "chars = data_loader.chars\n",
        "vocab = data_loader.vocab\n",
        "vocab_size = data_loader.vocab_size  # 전체 단어개수\n",
        "\n",
        "# 인풋데이터와 타겟데이터, 배치 사이즈를 입력받기 위한 플레이스홀더를 설정합니다.\n",
        "input_data = tf.placeholder(tf.int32, shape=[None, None])  # input_data : [batch_size, seq_length])\n",
        "target_data = tf.placeholder(tf.int32, shape=[None, None])  # target_data : [batch_size, seq_length])\n",
        "state_batch_size = tf.placeholder(tf.int32, shape=[])  # Training : 50, Sampling : 1\n",
        "\n",
        "# RNN의 마지막 히든레이어의 출력을 소프트맥스 출력값으로 변환해주기 위한 변수들을 선언합니다.\n",
        "# hidden_size -> vocab_size\n",
        "softmax_w = tf.Variable(tf.random_normal(shape=[hidden_size, vocab_size]), dtype=tf.float32)\n",
        "softmax_b = tf.Variable(tf.random_normal(shape=[vocab_size]), dtype=tf.float32)\n",
        "\n",
        "# num_hidden_layers만큼 LSTM cell(히든레이어)를 선언합니다.\n",
        "cells = []\n",
        "for _ in range(0, num_hidden_layers):\n",
        "    cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size)\n",
        "    cells.append(cell)\n",
        "\n",
        "# cell을 종합해서 RNN을 정의합니다.\n",
        "cell = tf.contrib.rnn.MultiRNNCell(cells, state_is_tuple=True)\n",
        "\n",
        "# 인풋데이터를 변환하기 위한 Embedding Matrix를 선언합니다.\n",
        "# vocab_size -> hidden_size\n",
        "embedding = tf.Variable(tf.random_normal(shape=[vocab_size, hidden_size]), dtype=tf.float32)\n",
        "inputs = tf.nn.embedding_lookup(embedding, input_data)\n",
        "\n",
        "# 초기 state 값을 0으로 초기화합니다.\n",
        "initial_state = cell.zero_state(state_batch_size, tf.float32)\n",
        "\n",
        "# 학습을 위한 tf.nn.dynamic_rnn을 선언합니다.\n",
        "# outputs : [batch_size, seq_length, hidden_size]\n",
        "outputs, final_state = tf.nn.dynamic_rnn(cell, inputs, initial_state=initial_state, dtype=tf.float32)\n",
        "# ouputs을 [batch_size * seq_length, hidden_size]] 형태로 바꿉니다.\n",
        "output = tf.reshape(outputs, [-1, hidden_size])\n",
        "\n",
        "# 최종 출력값을 설정합니다.\n",
        "# logits : [batch_size * seq_length, vocab_size]\n",
        "logits = tf.matmul(output, softmax_w) + softmax_b\n",
        "probs = tf.nn.softmax(logits)\n",
        "\n",
        "# Cross Entropy 손실 함수를 정의합니다.\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=target_data))\n",
        "\n",
        "# 옵티마이저를 선언하고 옵티마이저에 Gradient Clipping을 적용합니다.\n",
        "# grad_clip(=5)보다 큰 Gradient를 5로 Clippin합니다.\n",
        "tvars = tf.trainable_variables()\n",
        "grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "train_step = optimizer.apply_gradients(zip(grads, tvars))\n",
        "\n",
        "# 세션을 열고 학습을 진행합니다.\n",
        "with tf.Session() as sess:\n",
        "    # 변수들에 초기값을 할당합니다.\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for e in range(num_epochs):\n",
        "        data_loader.reset_batch_pointer()\n",
        "        # 초기 상태값을 지정합니다.\n",
        "        state = sess.run(initial_state, feed_dict={state_batch_size: batch_size})\n",
        "\n",
        "        for b in range(data_loader.num_batches):\n",
        "            # x, y 데이터를 불러옵니다.\n",
        "            x, y = data_loader.next_batch()\n",
        "            # y에 one_hot 인코딩을 적용합니다.\n",
        "            y = tf.one_hot(y, vocab_size)  # y : [batch_size, seq_length, vocab_size]\n",
        "            y = tf.reshape(y, [-1, vocab_size])  # y : [batch_size * seq_length, vocab_size]\n",
        "            y = y.eval()\n",
        "\n",
        "            # feed-dict에 사용할 값들과 LSTM 초기 cell state(feed_dict[c])값과 hidden layer 출력값(feed_dict[h])을 지정합니다.\n",
        "            feed_dict = {input_data: x, target_data: y, state_batch_size: batch_size}\n",
        "            for i, (c, h) in enumerate(initial_state):\n",
        "                feed_dict[c] = state[i].c\n",
        "                feed_dict[h] = state[i].h\n",
        "\n",
        "            # 한스텝 학습을 진행합니다.\n",
        "            _, loss_print, state = sess.run([train_step, loss, final_state], feed_dict=feed_dict)\n",
        "\n",
        "            print(\"{}(학습한 배치개수)/{}(학습할 배치개수), 반복(epoch): {}, 손실함수(loss): {:.3f}\".format(\n",
        "                e * data_loader.num_batches + b,\n",
        "                num_epochs * data_loader.num_batches,\n",
        "                (e + 1),\n",
        "                loss_print))\n",
        "\n",
        "    print(\"트레이닝이 끝났습니다!\")\n",
        "\n",
        "    # 샘플링 시작\n",
        "    print(\"샘플링을 시작합니다!\")\n",
        "    num_sampling = 4000  # 생성할 글자(Character)의 개수를 지정합니다.\n",
        "    prime = u' '  # 시작 글자를 ' '(공백)으로 지정합니다.\n",
        "    sampling_type = 1  # 샘플링 타입을 설정합니다.\n",
        "    state = sess.run(cell.zero_state(1, tf.float32))  # RNN의 최초 state값을 0으로 초기화합니다.\n",
        "\n",
        "\n",
        "    # Random Sampling을 위한 weighted_pick 함수를 정의합니다.\n",
        "    def weighted_pick(weights):\n",
        "        t = np.cumsum(weights)\n",
        "        s = np.sum(weights)\n",
        "        return (int(np.searchsorted(t, np.random.rand(1) * s)))\n",
        "\n",
        "\n",
        "    ret = prime  # 샘플링 결과를 리턴받을 ret 변수에 첫번째 글자를 할당합니다.\n",
        "    char = prime[-1]  # Char-RNN의 첫번쨰 인풋을 지정합니다.\n",
        "    for n in range(num_sampling):\n",
        "        x = np.zeros((1, 1))\n",
        "        x[0, 0] = vocab[char]\n",
        "\n",
        "        # RNN을 한스텝 실행하고 Softmax 행렬을 리턴으로 받습니다.\n",
        "        feed_dict = {input_data: x, state_batch_size: 1, initial_state: state}\n",
        "        [probs_result, state] = sess.run([probs, final_state], feed_dict=feed_dict)\n",
        "\n",
        "        # 불필요한 차원을 제거합니다.\n",
        "        # probs_result : (1,65) -> p : (65)\n",
        "        p = np.squeeze(probs_result)\n",
        "\n",
        "        # 샘플링 타입에 따라 3가지 종류로 샘플링 합니다.\n",
        "        # sampling_type : 0 -> 다음 글자를 예측할때 항상 argmax를 사용\n",
        "        # sampling_type : 1(defualt) -> 다음 글자를 예측할때 항상 random sampling을 사용\n",
        "        # sampling_type : 2 -> 다음 글자를 예측할때 이전 글자가 ' '(공백)이면 random sampling, 그렇지 않을 경우 argmax를 사용\n",
        "        if sampling_type == 0:\n",
        "            sample = np.argmax(p)\n",
        "        elif sampling_type == 2:\n",
        "            if char == ' ':\n",
        "                sample = weighted_pick(p)\n",
        "            else:\n",
        "                sample = np.argmax(p)\n",
        "        else:\n",
        "            sample = weighted_pick(p)\n",
        "\n",
        "        pred = chars[sample]\n",
        "        ret += pred  # 샘플링 결과에 현재 스텝에서 예측한 글자를 추가합니다. (예를들어 pred=L일 경우, ret = HEL -> HELL)\n",
        "        char = pred  # 예측한 글자를 다음 RNN의 인풋으로 사용합니다.\n",
        "\n",
        "    print(\"샘플링 결과:\")\n",
        "    print(ret)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "loading preprocessed files\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0722 09:11:50.306069 140078659954560 deprecation.py:323] From <ipython-input-2-bfa8002307b8>:43: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "W0722 09:11:52.942865 140078659954560 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0722 09:11:52.944671 140078659954560 deprecation.py:323] From <ipython-input-2-bfa8002307b8>:47: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "W0722 09:11:52.994972 140078659954560 deprecation.py:323] From <ipython-input-2-bfa8002307b8>:59: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "W0722 09:11:53.524725 140078659954560 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0722 09:11:53.542962 140078659954560 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:738: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0722 09:11:54.012252 140078659954560 deprecation.py:323] From <ipython-input-2-bfa8002307b8>:69: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "W0722 09:11:54.566354 140078659954560 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/clip_ops.py:286: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 5.609\n",
            "1(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 4.742\n",
            "2(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 4.004\n",
            "3(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 3.958\n",
            "4(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 3.654\n",
            "5(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 3.592\n",
            "6(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 3.484\n",
            "7(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 3.419\n",
            "8(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 3.341\n",
            "9(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 3.332\n",
            "10(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 3.259\n",
            "11(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 3.226\n",
            "12(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 3.147\n",
            "13(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 3.101\n",
            "14(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 3.115\n",
            "15(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 3.101\n",
            "16(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 3.090\n",
            "17(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 3.045\n",
            "18(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.986\n",
            "19(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.957\n",
            "20(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.973\n",
            "21(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.959\n",
            "22(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.901\n",
            "23(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.921\n",
            "24(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.882\n",
            "25(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.877\n",
            "26(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.802\n",
            "27(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.796\n",
            "28(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.747\n",
            "29(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.752\n",
            "30(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.771\n",
            "31(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.794\n",
            "32(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.658\n",
            "33(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.683\n",
            "34(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.666\n",
            "35(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.662\n",
            "36(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.716\n",
            "37(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.646\n",
            "38(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.618\n",
            "39(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.624\n",
            "40(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.635\n",
            "41(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.582\n",
            "42(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.541\n",
            "43(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.488\n",
            "44(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.579\n",
            "45(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.518\n",
            "46(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.466\n",
            "47(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.499\n",
            "48(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.506\n",
            "49(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.467\n",
            "50(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.455\n",
            "51(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.431\n",
            "52(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.433\n",
            "53(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.446\n",
            "54(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.419\n",
            "55(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.424\n",
            "56(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.359\n",
            "57(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.368\n",
            "58(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.351\n",
            "59(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.330\n",
            "60(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.407\n",
            "61(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.416\n",
            "62(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.359\n",
            "63(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.400\n",
            "64(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.339\n",
            "65(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.320\n",
            "66(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.329\n",
            "67(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.302\n",
            "68(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.332\n",
            "69(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.303\n",
            "70(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.260\n",
            "71(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.301\n",
            "72(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.277\n",
            "73(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.255\n",
            "74(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.242\n",
            "75(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.223\n",
            "76(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.244\n",
            "77(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.264\n",
            "78(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.271\n",
            "79(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.236\n",
            "80(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.268\n",
            "81(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.155\n",
            "82(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.229\n",
            "83(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.205\n",
            "84(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.233\n",
            "85(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.184\n",
            "86(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.159\n",
            "87(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.125\n",
            "88(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.248\n",
            "89(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.154\n",
            "90(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.123\n",
            "91(학습한 배치개수)/892(학습할 배치개수), 반복(epoch): 1, 손실함수(loss): 2.121\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}